{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logbook\n",
    "1. Read the data.\n",
    "2. Visualise the data.\n",
    "    1. Found that the features are all between 1 and 10.\n",
    "    2. Found missing data. Decided to just remove the row.\n",
    "    3. Thought of either using Neural Network or SVM.\n",
    "3. Develop a Neural Network.\n",
    "    - Stage 1: \n",
    "        1. Thought about the flow.\n",
    "        2. First made a one layer NN with linear activation.\n",
    "        3. Created one input/output pair with 5 features. \n",
    "        4. Trained the model and found that the weights were not what I want, but it worked. Then I realised that there was no way to find 5 unknowns with 1 equation.\n",
    "        5. Added more input/output pairs. \n",
    "        6. Trained the model and succeed! Finally created a one layer model with MSE loss and linear activation!! <-rubbish\n",
    "    - Stage 2:\n",
    "        1. Added one more layer, i.e. created a (5-3-1) model.\n",
    "        2. Found that I also need dL/dX but not juse dL/dW. So add that.\n",
    "        3. Found that I need to make use of matrices, instead of just multiplying values like a 1-output node layer.\n",
    "        4. Spent time to do the d_/d_, d_/d_, and d_/d_ on paper, and tried to figure out what the size of each matrix is.\n",
    "        5. Struggled with dot products but finally made it.\n",
    "        6. Created a two-layer regression model with MSE loss and linear activation!! <-which is equal to one layer NN with linear activation.\n",
    "        7. Just decided try to add one more layer (3 layers in total) but it didn't work.\n",
    "        8. Fixed the problem by dealing with the dot products again...\n",
    "        9. Successfully created a 3-layer regression model with MSE loss and linear activation!! <-function-wise it is still same as one layer\n",
    "    - Stage 3: \n",
    "        1. As we are supposed to make a classification NN but not a regression NN, so it's time to add sigmoid activation.\n",
    "        2. Added sigmoid activation function.\n",
    "        3. Worked on the derivative of the sigmoid function, and added d_activation_fucntion.\n",
    "        4. Refactored the code as I was ignoring this part due to the d(linear)/dx is 1.\n",
    "        5. Successfully created a 3-layer regression model with MSE loss and sigmoid-sigmoid-linear activations!! <-Due to the random weight initialisation, sometimes it never converges. \n",
    "    - Stage 4: \n",
    "        1. Classification NN uses cross-entropy instead of MSE. So time to add cross-entropy as a loss function.\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import log as ln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the data from the Internet and save it as csv\n",
    "The following cell only needs to run once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# # fetch dataset \n",
    "# breast_cancer_wisconsin_original = fetch_ucirepo(id=15) \n",
    "  \n",
    "# # data (as pandas dataframes) \n",
    "# X = breast_cancer_wisconsin_original.data.features \n",
    "# y = breast_cancer_wisconsin_original.data.targets \n",
    "\n",
    "# data = pd.concat([y, X], axis=1)\n",
    "# data.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data to X and y, and replace B and M (2&4) with 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are missing values. As we got a large dataset, we will just drop the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(683, 9) (683,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "data.dropna(inplace=True)\n",
    "X = data.drop(\"Class\", axis=1)\n",
    "y = data['Class']\n",
    "y.replace({2: 0, 4: 1}, inplace=True)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    num_of_layers = 0\n",
    "    def __init__(self, input_size, output_size, activation=\"linear\", random_state=None):\n",
    "        random.seed(random_state)\n",
    "        self.weights = random.rand(input_size+1, output_size)-0.5   # weight[0, x] is bias's weight\n",
    "        # self.weights[:,:] = 0.6\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "        if activation == \"linear\":\n",
    "            self.activation_function = lambda x: x\n",
    "            self.d_activation_function = lambda x: x * 1\n",
    "        elif activation == \"sigmoid\":\n",
    "            self.activation_function = lambda x: 1/(1+np.exp(-x))\n",
    "            self.d_activation_function = lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
    "        else:\n",
    "            raise Exception(\"Wrong activation function\")\n",
    "        Layer.num_of_layers += 1\n",
    "\n",
    "    def forward_propagation(self, input):   # input should be an array\n",
    "        input = np.concatenate(([[1], input]))    # add bias = 1\n",
    "        self.input = input\n",
    "        output = self.activation_function(input.dot(self.weights))\n",
    "        self.output = output\n",
    "        return output\n",
    "    \n",
    "    def backward_propagation(self, dl_dy, learning_rate = 0.005):\n",
    "        # print(f\"dl_dy.shape: {dl_dy.shape}\")\n",
    "        # print(f\"self.weights.shape: {self.weights.shape}\")\n",
    "\n",
    "        # dActivation/dWeights\n",
    "        da_dw = self.input.reshape(-1, 1)\n",
    "        # print(f\"da_dw.shape: {da_dw.shape}\")\n",
    "\n",
    "        dy_da = self.d_activation_function(self.input.dot(self.weights))\n",
    "        # print(f\"dy_da.shape: {dy_da.shape}\")\n",
    "\n",
    "        # dLoss/dWeights = dL/dy(previous) * dy/dActivation * dActivation/dWeights\n",
    "        # print(f\"dl_dy: {dl_dy}, dy_da: {dy_da}\")\n",
    "        dl_da = dl_dy * dy_da.reshape(-1, 1)\n",
    "        # print(f\"dl_da.shape: {dl_da.shape}\")\n",
    "        # dl_dw = da_dw.dot(self.d_activation_function(dl_dy.T))\n",
    "        dl_dw = da_dw.dot(dl_da.T)\n",
    "        # print(f\"dl_dw.shape: {dl_dw.shape}\")\n",
    "\n",
    "        # The previous layer needs this to do the back propagation\n",
    "        # dLoss/dX = dLoss/dy(previous) * dy/dActivation * dActivation/dX\n",
    "        # where dActivation/dX is just the weights without the bias' weight\n",
    "        dl_dx = (self.weights[1:]).dot(self.d_activation_function(dl_dy))\n",
    "        # print(f\"dl_dx.shape: {dl_dx.shape}\")\n",
    "\n",
    "        # update the weight using the gradient\n",
    "        self.weights -= learning_rate * dl_dw\n",
    "\n",
    "        # print()\n",
    "        # return the dL/dX, which will become the previous layer's dL/dy\n",
    "        return dl_dx\n",
    "        \n",
    "\n",
    "class Model:\n",
    "    def __init__(self, loss_function):\n",
    "        self.layers = []\n",
    "\n",
    "        # The reason of including MSE is I first developed a regression model\n",
    "        # to find the coefficients of a polynomial with linear activation.\n",
    "        if loss_function not in ['mse', 'cross_entropy']:\n",
    "            raise Exception(\"The provided loss function is not available.\")\n",
    "        \n",
    "        if loss_function == \"mse\":\n",
    "            self.d_loss_function = lambda y, pred: -2*(y-pred)\n",
    "        elif loss_function == \"cross_entropy\":\n",
    "            self.d_loss_function = lambda y, pred: -(y-pred)/(pred * (1-pred))\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def show_layers(self):\n",
    "        for i, l in enumerate(self.layers):\n",
    "            print(f\"Layer {i+1}: \", end=\"\")\n",
    "            print(f\"input size: {l.input_size}, output size: {l.output_size}\")\n",
    "\n",
    "    def predict(self, input):\n",
    "        prev_output = input\n",
    "        # loop through all the layers\n",
    "        for layer in self.layers:\n",
    "            # actually this should be output=fp(previous_output) --> previous_output=output\n",
    "            # but just want to make it a bit shorter\n",
    "            prev_output = layer.forward_propagation(prev_output)\n",
    "            # print(prev_output)\n",
    "        return prev_output\n",
    "\n",
    "    def train_one_cycle(self, input, expected_output, learning_rate = 1):\n",
    "        # Just caused me trouble once, so I add this checking\n",
    "        if len(input) != len(expected_output):\n",
    "            raise Exception(\"Input and Expected output have different length\")\n",
    "                \n",
    "        # Loop through each pair of input/output\n",
    "        for X, y in zip(input, expected_output):\n",
    "            # predict the output\n",
    "            pred = self.predict(X)\n",
    "\n",
    "            # print(self.layers[0].weights)\n",
    "\n",
    "            # error = (y - pred)\n",
    "            # derror_dpred = np.array([-2 * error])\n",
    "\n",
    "            # # The following is d(squared error)/d(prediction)\n",
    "            derror_dpred = np.array([self.d_loss_function(y, pred)])\n",
    "\n",
    "            # dl_dy means dLoss/dOutput, where output is the layer's output\n",
    "            dl_dy = derror_dpred\n",
    "            for layer in reversed(self.layers):\n",
    "                dl_dy = layer.backward_propagation(dl_dy, learning_rate)\n",
    "            # print(f\"X: {X}, y: {y}, pred: {pred}\")\n",
    "\n",
    "    def mean_squared_error(self, input, expected_output):\n",
    "        if len(input) != len(expected_output):\n",
    "            raise Exception(\"Input and Expected output have different length\")                \n",
    "        squared_error = 0\n",
    "        for X, y in zip(input, expected_output):    \n",
    "            squared_error += (y - self.predict(X))**2\n",
    "        mse = squared_error / len(input)\n",
    "        return mse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: input size: 2, output size: 1\n",
      "Input: [-2 -3], Expected output: 1, Predicted: [0.98991573]\n",
      "Input: [2 3], Expected output: 0, Predicted: [0.00552911]\n",
      "[[-0.30277075]\n",
      " [-1.06843708]\n",
      " [-0.91751318]]\n"
     ]
    }
   ],
   "source": [
    "input = np.array([[2, 3], [-2, -3]])\n",
    "expected_output = [0, 1]\n",
    "\n",
    "layer1 = Layer(2, 1, \"sigmoid\")\n",
    "layer2 = Layer(5, 1, \"sigmoid\")\n",
    "\n",
    "model = Model(\"cross_entropy\")\n",
    "model.add(layer1)\n",
    "# model.add(layer2)\n",
    "model.show_layers()\n",
    "\n",
    "for i in range(500):\n",
    "    model.train_one_cycle(input, expected_output, 0.01)\n",
    "\n",
    "print(f\"Input: {input[1]}, Expected output: {expected_output[1]}, Predicted: {model.predict(input[1])}\")\n",
    "print(f\"Input: {input[0]}, Expected output: {expected_output[0]}, Predicted: {model.predict(input[0])}\")\n",
    "print(f\"{model.layers[0].weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: input size: 5, output size: 5\n",
      "Layer 2: input size: 5, output size: 1\n",
      "[np.int64(75), np.int64(47), np.int64(88), np.int64(65), np.int64(104)]\n",
      "\n",
      "Input: [5 8 3 2 1] Expected output: 75 prediction: [87.69141924]\n",
      "Input: [4 2 1 5 6] Expected output: 47 prediction: [87.69543731]\n",
      "Input: [6 1 6 8 9] Expected output: 88 prediction: [87.69520404]\n",
      "Input: [7 4 3 2 1] Expected output: 65 prediction: [87.69517587]\n",
      "Input: [3 6 7 8 8] Expected output: 104 prediction: [87.69520356]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\psamt\\AppData\\Local\\Temp\\ipykernel_12012\\3054871977.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  self.activation_function = lambda x: 1/(1+np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "layer1 = Layer(5, 5, activation=\"sigmoid\")\n",
    "layer2 = Layer(5, 1, activation=\"linear\")\n",
    "\n",
    "\n",
    "model = Model(loss_function='mse')\n",
    "model.add(layer1)\n",
    "model.add(layer2)\n",
    "model.show_layers()\n",
    "\n",
    "input = np.array([[5, 8, 3, 2, 1], [4, 2, 1, 5, 6], [6, 1, 6, 8, 9], [7, 4, 3, 2, 1], [3, 6, 7, 8, 8]])\n",
    "expected_output = [(sum(i*[3, 4, 6, 2, 1])+5) for i in input]\n",
    "print(expected_output)\n",
    "\n",
    "print()\n",
    "for i in range(50):\n",
    "    model.train_one_cycle(input, expected_output, 0.001)\n",
    "    # print(model.mean_squared_error(input, expected_output))\n",
    "\n",
    "for i in range(len(input)):\n",
    "    print(f\"Input: {input[i]} Expected output: {expected_output[i]} prediction: {model.predict(input[i])}\")\n",
    "    # print(f\"Input: {input[i]} Expected output: {expected_output[i]} prediction: {model.predict(input[i])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLiSP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
